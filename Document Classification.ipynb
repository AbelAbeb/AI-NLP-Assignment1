{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "199f8282",
   "metadata": {},
   "source": [
    "# Wikipedia Text Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba01099",
   "metadata": {},
   "source": [
    "This project involves classifying Wikipedia documents into medical and non-medical categories using machine learning models. The pipeline includes data retrieval, preprocessing, model training, and evaluation. Details of the code is described in the comment of the code and readme file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fda9734c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing required libreries\n",
    "\n",
    "import joblib \n",
    "import numpy as np\n",
    "import nltk\n",
    "import requests\n",
    "import pandas as pd\n",
    "from sklearn.metrics import precision_score, f1_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer, WordNetLemmatizer\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9dcea0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Abel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Abel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                Text  Label\n",
      "0  Medical literature is the scientific literatur...      1\n",
      "1  The Brandonâ€“Hill list is a list of journals an...      1\n",
      "2  In medicine, a case report is a detailed repor...      1\n",
      "3  A Counterblaste to Tobacco is a treatise writt...      1\n",
      "4  Crossing the Quality Chasm: A New Health Syste...      1\n",
      "Naive Bayes Cross-Validation Scores: [0.90429043 0.92409241 0.91721854 0.90728477 0.92384106]\n",
      "Logistic Regression Cross-Validation Scores: [0.89768977 0.94719472 0.92715232 0.91721854 0.95033113]\n",
      "Average Naive Bayes Cross-Validation Score: 0.9153454418289512\n",
      "Average Logistic Regression Cross-Validation Score: 0.9279172950407624\n",
      "      Metric  Naive Bayes  Logistic Regression\n",
      "0   Accuracy     0.926121             0.931398\n",
      "1  Precision     0.931559             0.952569\n",
      "2   F1 Score     0.945946             0.948819\n"
     ]
    }
   ],
   "source": [
    "# Download NLTK resources\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "def get_documents_from_category(category_title, max_documents=100):\n",
    "    base_url = \"https://en.wikipedia.org/w/api.php\"\n",
    "    params = {\n",
    "        \"action\": \"query\",\n",
    "        \"format\": \"json\",\n",
    "        \"list\": \"categorymembers\",\n",
    "        \"cmtitle\": category_title,\n",
    "        \"cmlimit\": max_documents\n",
    "    }\n",
    "\n",
    "    response = requests.get(base_url, params=params)\n",
    "    data = response.json()\n",
    "    \n",
    "    documents = []\n",
    "    # Check if the response contains category members\n",
    "    if \"query\" in data and \"categorymembers\" in data[\"query\"]:\n",
    "        for member in data[\"query\"][\"categorymembers\"]:\n",
    "            page_title = member[\"title\"]\n",
    "            document_text = get_wikipedia_text(page_title)\n",
    "            if document_text.strip():\n",
    "                documents.append(document_text)\n",
    "       \n",
    "    return documents\n",
    "\n",
    "def get_wikipedia_text(title):\n",
    "    base_url = \"https://en.wikipedia.org/w/api.php\"\n",
    "    params = {\n",
    "        \"action\": \"query\",\n",
    "        \"format\": \"json\",\n",
    "        \"titles\": title,\n",
    "        \"prop\": \"extracts\",\n",
    "        \"exintro\": True\n",
    "    }\n",
    "\n",
    "    response = requests.get(base_url, params=params)\n",
    "    data = response.json()\n",
    "    # Check if the page exists\n",
    "    if \"query\" in data and \"pages\" in data[\"query\"]:\n",
    "        page_id = list(data[\"query\"][\"pages\"].keys())[0]\n",
    "\n",
    "        # Check if the page has an extract\n",
    "        if \"extract\" in data[\"query\"][\"pages\"][page_id]:\n",
    "            return data[\"query\"][\"pages\"][page_id][\"extract\"]\n",
    "\n",
    "    return \"\"\n",
    "\n",
    "def preprocess_text(text, use_stopwords=False, use_stemming=False, use_lemmatization=False):\n",
    "    # Remove HTML tags\n",
    "    text = BeautifulSoup(text, 'html.parser').get_text()\n",
    "    text1 = text\n",
    "    # Tokenization\n",
    "    words = nltk.word_tokenize(text.lower())\n",
    "    \n",
    "    # Remove non-alphabetic characters\n",
    "    words = [word for word in words if word.isalpha()]\n",
    "\n",
    "    # remove stopwords\n",
    "    if use_stopwords:\n",
    "        stop_words = set(stopwords.words(\"english\"))\n",
    "        words = [word for word in words if word not in stop_words]\n",
    "\n",
    "    # apply stemming\n",
    "    if use_stemming:\n",
    "        stemmer = SnowballStemmer(\"english\")\n",
    "        words = [stemmer.stem(word) for word in words]\n",
    "\n",
    "    # apply lemmatization\n",
    "    if use_lemmatization:\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    return \" \".join(words), text1\n",
    "\n",
    "# Fetch documents from multiple Wikipedia categories\n",
    "medical_categories = [\n",
    "    \"Category:Medical literature\",\n",
    "    \"Category:Medicine\",\n",
    "    \"Category:Health\",\n",
    "    \"Category:Anatomy\",\n",
    "    \"Category:Diseases\",\n",
    "    \"Category:Medical treatments\",\n",
    "    \"Category:Oncology\",\n",
    "    \"Category:Pediatrics\",\n",
    "    \"Category:Pharmacology\",\n",
    "    \"Category:Nursing\",\n",
    "    \"Category:Public_health\",\n",
    "    \"Category:Surgery\",\n",
    "    \"Category:Medical_diagnosis\",\n",
    "    \"Category:Genetics\",\n",
    "    \"Category:Neurology\",\n",
    "    \"Category:Psychiatry\",\n",
    "    \"Category:Immunology\",\n",
    "    \"Category:Cardiology\"\n",
    "]\n",
    "\n",
    "non_medical_categories = [\n",
    "    \"Category:Science\",\n",
    "    \"Category:History\",\n",
    "    \"Category:Arts\",\n",
    "    \"Category:Geography\",\n",
    "    \"Category:Technology\",\n",
    "    \"Category:Sport\",\n",
    "    \"Category:Computing\",\n",
    "    \"Category:Entertainment\",\n",
    "    \"Category:Business\",\n",
    "    \"Category:Food_and_drink\",\n",
    "    \"Category:Philosophy\",\n",
    "    \"Category:Literature\",\n",
    "    \"Category:Music\",\n",
    "    \"Category:Politics\",\n",
    "    \"Category:Economics\",\n",
    "    \"Category:Religion\"\n",
    "]\n",
    "\n",
    "# Fetch documents from medical categories\n",
    "medical_documents = []\n",
    "for category in medical_categories:\n",
    "    medical_documents.extend(get_documents_from_category(category, max_documents=100))\n",
    "\n",
    "# Fetch documents from non-medical categories\n",
    "non_medical_documents = []\n",
    "for category in non_medical_categories:\n",
    "    non_medical_documents.extend(get_documents_from_category(category, max_documents=100))\n",
    "\n",
    "# Preprocess the texts\n",
    "\n",
    "preprocessed_medical_documents = [preprocess_text(doc, use_stopwords=True, use_stemming=True) for doc in medical_documents if doc.strip()]\n",
    "preprocessed_non_medical_documents = [preprocess_text(doc, use_stopwords=True, use_stemming=True) for doc in non_medical_documents if doc.strip()]\n",
    "\n",
    "# Extract the processed text and the original text separately\n",
    "preprocessed_medical_documents, CSV1 = zip(*preprocessed_medical_documents)\n",
    "preprocessed_non_medical_documents, CSV2 = zip(*preprocessed_non_medical_documents)\n",
    "\n",
    "\n",
    "# Create a dataset with labels (1 for medical, 0 for non-medical)\n",
    "csv = CSV1 + CSV2\n",
    "documents = preprocessed_medical_documents + preprocessed_non_medical_documents\n",
    "labels = [1] * len(preprocessed_medical_documents) + [0] * len(preprocessed_non_medical_documents)\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame({'Text': csv, 'Label': labels})\n",
    "\n",
    "# Save DataFrame to CSV\n",
    "df.to_csv('wikipedia_documents_labels.csv', index=False)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df.head())\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(documents, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a pipeline with CountVectorizer and Naive Bayes\n",
    "nb_model = make_pipeline(CountVectorizer(), MultinomialNB())\n",
    "\n",
    "# Create a pipeline with CountVectorizer and Logistic Regression\n",
    "lr_model = make_pipeline(CountVectorizer(), LogisticRegression())\n",
    "\n",
    "# Perform cross-validation on the training set\n",
    "nb_cv_scores = cross_val_score(nb_model, X_train, y_train, cv=5, scoring='accuracy')\n",
    "lr_cv_scores = cross_val_score(lr_model, X_train, y_train, cv=5, scoring='accuracy')\n",
    "\n",
    "# Display cross-validation scores\n",
    "print(\"Naive Bayes Cross-Validation Scores:\", nb_cv_scores)\n",
    "print(\"Logistic Regression Cross-Validation Scores:\", lr_cv_scores)\n",
    "\n",
    "# Average cross-validation scores\n",
    "print(\"Average Naive Bayes Cross-Validation Score:\", np.mean(nb_cv_scores))\n",
    "print(\"Average Logistic Regression Cross-Validation Score:\", np.mean(lr_cv_scores))\n",
    "\n",
    "# Train the models on the entire training set\n",
    "nb_model.fit(X_train, y_train)\n",
    "lr_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the models on the test set\n",
    "nb_test_predictions = nb_model.predict(X_test)\n",
    "lr_test_predictions = lr_model.predict(X_test)\n",
    "\n",
    "nb_test_accuracy = accuracy_score(y_test, nb_test_predictions)\n",
    "lr_test_accuracy = accuracy_score(y_test, lr_test_predictions)\n",
    "\n",
    "# Calculate precision and F1 Score\n",
    "nb_precision = precision_score(y_test, nb_test_predictions)\n",
    "nb_f1_score = f1_score(y_test, nb_test_predictions)\n",
    "\n",
    "lr_precision = precision_score(y_test, lr_test_predictions)\n",
    "lr_f1_score = f1_score(y_test, lr_test_predictions)\n",
    "\n",
    "\n",
    "# Create a DataFrame\n",
    "results_df = pd.DataFrame({\n",
    "    'Metric': ['Accuracy', 'Precision', 'F1 Score'],\n",
    "    'Naive Bayes': [nb_test_accuracy, nb_precision, nb_f1_score],\n",
    "    'Logistic Regression': [lr_test_accuracy, lr_precision, lr_f1_score]\n",
    "})\n",
    "\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "970f07a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Prediction: Medical\n",
      "Logistic Regression Prediction: Medical\n"
     ]
    }
   ],
   "source": [
    "# Save trained models to files\n",
    "joblib.dump(nb_model, 'naive_bayes_model.joblib')\n",
    "joblib.dump(lr_model, 'logistic_regression_model.joblib')\n",
    "\n",
    "# Load models from files\n",
    "nb_model_loaded = joblib.load('naive_bayes_model.joblib')\n",
    "lr_model_loaded = joblib.load('logistic_regression_model.joblib')\n",
    "\n",
    "# sample custom text inputs to be predicted\n",
    "custom_text = \"Immunology is the study of the immune system and its functions. It plays a crucial role in protecting the body from infections and diseases. This field explores the intricacies of the immune response, including the role of white blood cells and antibodies.\"\n",
    "\n",
    "# Preprocess the custom text\n",
    "preprocessed_custom_text, _ = preprocess_text(custom_text, use_stopwords=True, use_stemming=True)\n",
    "\n",
    "# Use the models to make predictions\n",
    "nb_prediction = nb_model_loaded.predict([preprocessed_custom_text])[0]\n",
    "lr_prediction = lr_model_loaded.predict([preprocessed_custom_text])[0]\n",
    "\n",
    "# Display predictions\n",
    "\n",
    "#print(\"Naive Bayes Prediction:\", nb_prediction)\n",
    "#print(\"Logistic Regression Prediction:\", lr_prediction)\n",
    "\n",
    "if nb_prediction == 1:\n",
    "    print(\"Naive Bayes Prediction: Medical\")\n",
    "else:\n",
    "    print(\"Naive Bayes Prediction: Non-Medical\")\n",
    "\n",
    "if lr_prediction == 1:\n",
    "    print(\"Logistic Regression Prediction: Medical\")\n",
    "else:\n",
    "    print(\"Logistic Regression Prediction: Non-Medical\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5152f9f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
