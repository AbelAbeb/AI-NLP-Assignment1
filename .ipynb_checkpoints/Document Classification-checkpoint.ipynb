{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "199f8282",
   "metadata": {},
   "source": [
    "# Building medical and non-medical document classifier model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1c42af93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Abel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Abel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                Text  Label\n",
      "0  medic literatur scientif literatur medicin art...      1\n",
      "1  list list journal book recommend small hospit ...      1\n",
      "2  medicin case report detail report symptom sign...      1\n",
      "3  counterblast tobacco treatis written king jame...      1\n",
      "4  cross qualiti chasm new health system centuri ...      1\n",
      "Naive Bayes Cross-Validation Scores: [0.91419142 0.92739274 0.93069307 0.92409241 0.91721854]\n",
      "Logistic Regression Cross-Validation Scores: [0.90759076 0.95379538 0.93069307 0.92079208 0.9205298 ]\n",
      "Average Naive Bayes Cross-Validation Score: 0.9227176360020108\n",
      "Average Logistic Regression Cross-Validation Score: 0.9266802176906431\n",
      "      Metric  Naive Bayes  Logistic Regression\n",
      "0   Accuracy     0.918206             0.936675\n",
      "1  Precision     0.929104             0.964706\n",
      "2   F1 Score     0.941399             0.953488\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, f1_score\n",
    "import numpy as np\n",
    "import nltk\n",
    "import requests\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer, WordNetLemmatizer\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "def get_documents_from_category(category_title, max_documents=100):\n",
    "    base_url = \"https://en.wikipedia.org/w/api.php\"\n",
    "    params = {\n",
    "        \"action\": \"query\",\n",
    "        \"format\": \"json\",\n",
    "        \"list\": \"categorymembers\",\n",
    "        \"cmtitle\": category_title,\n",
    "        \"cmlimit\": max_documents\n",
    "    }\n",
    "\n",
    "    response = requests.get(base_url, params=params)\n",
    "    data = response.json()\n",
    "    \n",
    "    documents = []\n",
    "    # Check if the response contains category members\n",
    "    if \"query\" in data and \"categorymembers\" in data[\"query\"]:\n",
    "        for member in data[\"query\"][\"categorymembers\"]:\n",
    "            page_title = member[\"title\"]\n",
    "            #print(\"page_title\",page_title)\n",
    "            document_text = get_wikipedia_text(page_title)\n",
    "            #print(document_text)\n",
    "            if document_text.strip():\n",
    "                documents.append(document_text)\n",
    "       \n",
    "    return documents\n",
    "\n",
    "def get_wikipedia_text(title):\n",
    "    base_url = \"https://en.wikipedia.org/w/api.php\"\n",
    "    params = {\n",
    "        \"action\": \"query\",\n",
    "        \"format\": \"json\",\n",
    "        \"titles\": title,\n",
    "        \"prop\": \"extracts\",\n",
    "        \"exintro\": True\n",
    "    }\n",
    "\n",
    "    response = requests.get(base_url, params=params)\n",
    "    data = response.json()\n",
    "    #print(\"json data\", data) \n",
    "    # Check if the page exists\n",
    "    if \"query\" in data and \"pages\" in data[\"query\"]:\n",
    "        page_id = list(data[\"query\"][\"pages\"].keys())[0]\n",
    "\n",
    "        # Check if the page has an extract\n",
    "        if \"extract\" in data[\"query\"][\"pages\"][page_id]:\n",
    "            return data[\"query\"][\"pages\"][page_id][\"extract\"]\n",
    "\n",
    "    return \"\"\n",
    "\n",
    "def preprocess_text(text, use_stopwords=False, use_stemming=False, use_lemmatization=False):\n",
    "    # Remove HTML tags\n",
    "    text = BeautifulSoup(text, 'html.parser').get_text()\n",
    "    #print(\"text\", text)\n",
    "    # Tokenization\n",
    "    words = nltk.word_tokenize(text.lower())\n",
    "\n",
    "    # Remove non-alphabetic characters\n",
    "    words = [word for word in words if word.isalpha()]\n",
    "\n",
    "    # remove stopwords\n",
    "    if use_stopwords:\n",
    "        stop_words = set(stopwords.words(\"english\"))\n",
    "        words = [word for word in words if word not in stop_words]\n",
    "\n",
    "    # apply stemming\n",
    "    if use_stemming:\n",
    "        stemmer = SnowballStemmer(\"english\")\n",
    "        words = [stemmer.stem(word) for word in words]\n",
    "\n",
    "    # apply lemmatization\n",
    "    if use_lemmatization:\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        words = [lemmatizer.lemmatize(word) for word in words]\n",
    "\n",
    "    return \" \".join(words)\n",
    "\n",
    "# Fetch documents from multiple Wikipedia categories\n",
    "medical_categories = [\n",
    "    \"Category:Medical literature\",\n",
    "    \"Category:Medicine\",\n",
    "    \"Category:Health\",\n",
    "    \"Category:Anatomy\",\n",
    "    \"Category:Diseases\",\n",
    "    \"Category:Medical treatments\",\n",
    "    \"Category:Oncology\",\n",
    "    \"Category:Pediatrics\",\n",
    "    \"Category:Pharmacology\",\n",
    "    \"Category:Nursing\",\n",
    "    \"Category:Public_health\",\n",
    "    \"Category:Surgery\",\n",
    "    \"Category:Medical_diagnosis\",\n",
    "    \"Category:Genetics\",\n",
    "    \"Category:Neurology\",\n",
    "    \"Category:Psychiatry\",\n",
    "    \"Category:Immunology\",\n",
    "    \"Category:Cardiology\"\n",
    "]\n",
    "\n",
    "non_medical_categories = [\n",
    "    \"Category:Science\",\n",
    "    \"Category:History\",\n",
    "    \"Category:Arts\",\n",
    "    \"Category:Geography\",\n",
    "    \"Category:Technology\",\n",
    "    \"Category:Sport\",\n",
    "    \"Category:Computing\",\n",
    "    \"Category:Entertainment\",\n",
    "    \"Category:Business\",\n",
    "    \"Category:Food_and_drink\",\n",
    "    \"Category:Philosophy\",\n",
    "    \"Category:Literature\",\n",
    "    \"Category:Music\",\n",
    "    \"Category:Politics\",\n",
    "    \"Category:Economics\",\n",
    "    \"Category:Religion\"\n",
    "]\n",
    "\n",
    "# Fetch documents from medical categories\n",
    "medical_documents = []\n",
    "for category in medical_categories:\n",
    "    medical_documents.extend(get_documents_from_category(category, max_documents=100))\n",
    "\n",
    "# Fetch documents from non-medical categories\n",
    "non_medical_documents = []\n",
    "for category in non_medical_categories:\n",
    "    non_medical_documents.extend(get_documents_from_category(category, max_documents=100))\n",
    "\n",
    "# Preprocess the texts\n",
    "preprocessed_medical_documents = [preprocess_text(doc, use_stopwords=True, use_stemming=True) for doc in medical_documents if doc.strip()]\n",
    "preprocessed_non_medical_documents = [preprocess_text(doc, use_stopwords=True, use_stemming=True) for doc in non_medical_documents if doc.strip()]\n",
    "\n",
    "# Create a dataset with labels (1 for medical, 0 for non-medical)\n",
    "documents = preprocessed_medical_documents + preprocessed_non_medical_documents\n",
    "labels = [1] * len(preprocessed_medical_documents) + [0] * len(preprocessed_non_medical_documents)\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame({'Text': documents, 'Label': labels})\n",
    "\n",
    "# Save DataFrame to CSV\n",
    "df.to_csv('wikipedia_documents_labels.csv', index=False)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df.head())\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(documents, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a pipeline with CountVectorizer and Naive Bayes\n",
    "nb_model = make_pipeline(CountVectorizer(), MultinomialNB())\n",
    "\n",
    "# Create a pipeline with CountVectorizer and Logistic Regression\n",
    "lr_model = make_pipeline(CountVectorizer(), LogisticRegression())\n",
    "\n",
    "# Perform cross-validation on the training set\n",
    "nb_cv_scores = cross_val_score(nb_model, X_train, y_train, cv=5, scoring='accuracy')\n",
    "lr_cv_scores = cross_val_score(lr_model, X_train, y_train, cv=5, scoring='accuracy')\n",
    "\n",
    "# Display cross-validation scores\n",
    "print(\"Naive Bayes Cross-Validation Scores:\", nb_cv_scores)\n",
    "print(\"Logistic Regression Cross-Validation Scores:\", lr_cv_scores)\n",
    "\n",
    "# Average cross-validation scores\n",
    "print(\"Average Naive Bayes Cross-Validation Score:\", np.mean(nb_cv_scores))\n",
    "print(\"Average Logistic Regression Cross-Validation Score:\", np.mean(lr_cv_scores))\n",
    "\n",
    "# Train the models on the entire training set\n",
    "nb_model.fit(X_train, y_train)\n",
    "lr_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the models on the test set\n",
    "nb_test_predictions = nb_model.predict(X_test)\n",
    "lr_test_predictions = lr_model.predict(X_test)\n",
    "\n",
    "nb_test_accuracy = accuracy_score(y_test, nb_test_predictions)\n",
    "lr_test_accuracy = accuracy_score(y_test, lr_test_predictions)\n",
    "\n",
    "# Calculate precision and F1 Score\n",
    "nb_precision = precision_score(y_test, nb_test_predictions)\n",
    "nb_f1_score = f1_score(y_test, nb_test_predictions)\n",
    "\n",
    "lr_precision = precision_score(y_test, lr_test_predictions)\n",
    "lr_f1_score = f1_score(y_test, lr_test_predictions)\n",
    "\n",
    "import pandas as pd\n",
    "# Create a DataFrame\n",
    "results_df = pd.DataFrame({\n",
    "    'Metric': ['Accuracy', 'Precision', 'F1 Score'],\n",
    "    'Naive Bayes': [nb_test_accuracy, nb_precision, nb_f1_score],\n",
    "    'Logistic Regression': [lr_test_accuracy, lr_precision, lr_f1_score]\n",
    "})\n",
    "\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9dcea0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970f07a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
